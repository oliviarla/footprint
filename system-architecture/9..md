# 9. 웹 크롤러

## 💬 웹 크롤러란

* 검색 엔진에서 널리 쓰는 기술로, 웹에 새로 올라오거나 갱신된 콘텐츠를 찾아내 수집하는 목적으로 사용된다.

### 활용

* 검색 엔진 인덱싱
  * 웹 페이지를 모아 검색 엔진을 위한 로컬 인덱스를 만든다.
  * Googlebot은 구글의 검색 엔진을 위한 웹 크롤러이다.
* 웹 아카이빙
  * 장기 보관을 위해 웹에서 정보를 모으는 것
  * 국립 도서관들이 크롤러를 돌려 웹사이트를 아카이빙하기도 한다.
* 웹 마이닝
  * 웹 사이트를 통해 데이터를 마이닝하여 유용한 지식을 도출해내는 것
* 웹 모니터링
  * 저작권이나 상표권이 침해되는 사례를 모니터링하는 것

### 크롤러의 조건

* **규모 확장성**: 웹에는 수십억 개의 페이지가 존재한다. 따라서 병행성(parallelism)을 활용하여 효율적으로 웹 크롤링을 하면 좋다.
* **안정성**(robustness): 웹에는 잘못 작성된 HTML, 아무 반응이 없는 서버, 장애, 악성 코드가 붙어 있는 링크 등이 존재한다. 크롤러는 이런 비정상적 입력이나 환경에 잘 대응할 수 있어야 한다.
* **예절**(politeness): 수집 대상 웹 사이트에 짧은 시간 동안 너무 많은 요청을 보내서는 안 된다.
* **확장성**(extensibility): 새로운 형태의 콘텐츠를 지원하기가 쉬워야 한다. 이미지 파일 등을 크롤링해야 할 때에 시스템이 확장하기 쉬워야 한다.

## 💬 컴포넌트

<figure><img src="../.gitbook/assets/image (38).png" alt=""><figcaption></figcaption></figure>

#### 시작 URL 집합

* 크롤러가 가능한 한 많은 링크를 탐색할 수 있도록 URL을 골라야 한다.
* 주제별로 세분화하여 서로 다른 시작 URL을 사용해도 된다.

#### 미수집 URL 저장소

* 이미 다운로드되지 않은 미수집 URL을 저장한다.
* 동일 웹사이트에 대해서는 한 번에 한 페이지만 요청해야 한다.
* 수많은 URL 데이터를 메모리에만 저장하면 안정성, 규모 확장성 측면에서 적합하지 않다. 대부분의 URL은 디스크에 저장하고 IO 비용을 줄이기 위해 메모리 버퍼에 큐를 두고 주기적으로 디스크에 기록되도록 해야 한다.
* **큐 라우터**를 통해 동일 호스트를 가진 URL은 항상 하나의 큐에 입력되도록 하고, **큐 선택기**를 통해 스레드가 큐에 있던 URL을 하나 꺼내 수행하도록 할 수 있다. 작업 스레드는 URL을 다운받게 되고, 다음 작업을 진행하기 전 일정 시간동안 대기(delay) 시킬 수 있다.

<figure><img src="../.gitbook/assets/image (39).png" alt=""><figcaption></figcaption></figure>

* URL 우선 순위
  * PageRank, 트래픽 양, 갱신 빈도 등의 기준을 통해 URL의 우선 순위를 결정할 수 있다.
  * 우선순위별로 큐를 할당하여 우선 순위가 높은 큐부터 큐 선택기가 선택할 수 있도록 한다.
  * 이렇게 우선순위를 기반으로 선택된 URL는 앞서 다뤘던 큐 라우터에 전달된다.

<figure><img src="../.gitbook/assets/image (41).png" alt=""><figcaption></figcaption></figure>

* 재수집
  * 기존에 다운로드했던 URL도 업데이트될 수 있기 때문에 주기적으로 재수집할 필요가 있다.
  * 웹 페이지의 변경 이력을 활용하거나, 중요한 페이지는 우선 순위를 높여 자주 재수집되도록 해야 한다.

#### HTML 다운로더

* 웹 페이지를 다운로드한다.
* Robots.txt 파일에 수집 가능한 페이지 목록이 들어 있으므로 이 파일을 주기적으로 캐싱해 사용해야 한다.
* 성능 최적화를 위한 기법은 다음과 같다.
  * 분산 크롤링
    * 여러 서버에 크롤링 작업을 분산
    * 각 서버는 여러 스레드 돌려 다운로드 작업 처리
  * 도메인 이름 변환 결과 캐싱
    * 한 스레드에서 DNS 요청할 경우, 결과를 받기 전까지 다음 작업 진행이 불가능하다.
    * 크롤링 스레드 중 하나라도 이 작업을 하고 있으면 다른 스레드의 DNS 요청도 전부 블록되므로 병목 현상이 발생한다.
    * DNS 요청 처리는 보통 10ms\~200ms 소요된다.
    * 캐시에 도메인 이름, IP 주소 보관해두고 주기적으로 갱신을 해주어야 한다.
  * 지역성을 활용해 크롤링 서버를 지역별로 분산시키고, 크롤링 서버는 지역적으로 가까운 크롤링 대상 서버를 다루도록 한다. 이를 통해 페이지 다운로드 시간을 절감할 수 있다.
  * timeout을 짧게 설정해 응답하지 않는 서버는 넘어가야 한다.
* 안정성을 위해 다음 방법들을 활용할 수 있다.
  * &#x20;consistent hashing 적용하여 다운로더 서버들에 들어가는 부하를 분산시킨다.
  * 장애가 발생해도 쉽게 복구할 수 있도록 데이터를 지속적 저장장치에 저장한다.
  * 예외가 발생해도 전체 시스템이 중단되지 않도록 예외 처리를 우아하게 해야 한다.
  * 시스템 오류를 방지하기 위해 데이터 검증을 해야 한다.
* 확장성을 위해 새로운 형태의 콘텐츠를 지원하는 모듈을 쉽게 추가할 수 있어야 한다.

#### 도메인 이름 변환기

* URL을 IP 주소로 변환 후 데이터를 요청하기 위해 필요하다.

#### 콘텐츠 파서

* 웹 페이지 다운 시 파싱과 검증 절차를 거치도록 한다.

#### 중복 컨텐츠

* 데이터 중복을 줄이고 기존에 저장된 콘텐츠임을 쉽게 알아내야 한다.
* 웹 페이지의 해시 값이나 체크섬을 이용해 비교하는 방법이 있다.

#### 콘텐츠 저장소

* HTML 문서를 보관한다.
* 저장할 데이터의 유형, 크기, 접근 빈도, 유효 기간을 고려해야 한다.
* 대부분의 콘텐츠는 디스크에 저장하고, 자주 접근되는 콘텐츠는 메모리에 둘 수 있다.

#### URL 추출기

* HTML 페이지를 파싱해 URL을 추출한다.
* 상대 경로는 절대 경로로 변환해 저장해야 한다.

#### URL 필터

* 특정 콘텐츠 타입이나 파일 확장자를 갖는 URL, 접속 시 오류가 발생하는 URL, 접근 제외 목록에 포함된 URL 등을 크롤링 대상에서 제외한다.
* 크롤러가 무한 루프에 빠질 수 있는 무한히 깊은 디렉토리 구조를 포함한 URL의 경우, URL의 최대 길이를 제한하거나 수작업으로 확인해 크롤링 대상에서 제외해야 한다.

#### 방문 여부

* URL에 이미 방문했는지, 이미 미수집 URL 저장소에 있는지 확인해야 한다.
* 블룸 필터나 해시 테이블을 이용해 빠르게 탐지할 수 있다.

#### URL 저장소

* 이미 방문한 URL을 저장한다.
